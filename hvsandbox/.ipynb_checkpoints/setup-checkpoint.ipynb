{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on a single node\n",
    "\n",
    "## Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_folder = './train'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "from absl import flags\n",
    "from absl import app\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, add\n",
    "\n",
    "import horovod.tensorflow.keras as hvd\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "IMAGE_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "def toy_resnet_model():\n",
    "    inputs = Input(shape=IMAGE_SHAPE, name='image')\n",
    "    x = Conv2D(32, 3, activation='relu')(inputs)\n",
    "    x = Conv2D(64, 3, activation='relu')(x)\n",
    "    block_1_output = MaxPooling2D(3)(x)\n",
    "    \n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(block_1_output)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    block_2_output = add([x, block_1_output])\n",
    "    \n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    block_3_output = add([x, block_2_output])\n",
    "    \n",
    "    x = Conv2D(64, 3, activation='relu')(block_3_output)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='toy_resnet')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_datasets():\n",
    "    def _parse_record(example_proto):\n",
    "        features = {\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64, default_value=0)\n",
    "        }\n",
    "        \n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        image = parsed_features['image']\n",
    "        label = parsed_features['label']\n",
    "        \n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = image / 255\n",
    "        \n",
    "        label = tf.one_hot(label, NUM_CLASSES)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    \n",
    "    train_dataset = tf.data.TFRecordDataset(FLAGS.train_files)\n",
    "    eval_dataset = tf.data.TFRecordDataset(FLAGS.eval_files)\n",
    "    \n",
    "    train_dataset = train_dataset.map(_parse_record)\n",
    "    eval_dataset = eval_dataset.map(_parse_record)\n",
    "    \n",
    "    train_dataset = train_dataset.shuffle(4096).batch(FLAGS.batch_size).repeat()\n",
    "    eval_dataset = eval_dataset.batch(FLAGS.batch_size).repeat()\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "\n",
    "def train_evaluate():\n",
    "    \n",
    "    # Initialize Horovod\n",
    "    hvd.init()\n",
    "    \n",
    "    # Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "    tf.keras.backend.set_session(tf.Session(config=config))\n",
    "    \n",
    "    train_dataset, eval_dataset = prepare_datasets()\n",
    "    \n",
    "    model = toy_resnet_model()\n",
    "    \n",
    "    # Wrap an optimizer in Horovod\n",
    "    optimizer = hvd.DistributedOptimizer(optimizers.Adadelta())\n",
    "  \n",
    "    model.compile(optimizer=optimizer,\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "    callbacks = [\n",
    "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "        # This is necessary to ensure consistent initialization of all workers when\n",
    "        # training is started with loaded weights.\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "        # Horovod: average metrics among workers at the end of every epoch.\n",
    "        #\n",
    "        # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "        # TensorBoard, or other metrics-based callbacks.\n",
    "        hvd.callbacks.MetricAverageCallback()\n",
    "    ]\n",
    "    \n",
    "    # Horovod: save checkpoints only on worker 0 (master) to prevent other workers from corrupting them.\n",
    "    # Configure Tensorboard and Azure ML Tracking\n",
    "    if hvd.rank() == 0:\n",
    "        #callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=FLAGS['job-dir'].value, update_freq='epoch'))\n",
    "    \n",
    "    model.fit(train_dataset,\n",
    "         epochs=FLAGS.epochs,\n",
    "         steps_per_epoch=1000,\n",
    "         callbacks=callbacks,\n",
    "         validation_data=eval_dataset,\n",
    "         validation_steps=200)                    \n",
    "    \n",
    "    # Save the trained model to outputs folder on the master\n",
    "    #if hvd.rank() == 0:  \n",
    "    #    print(\"Training completed.\")\n",
    "    #    os.makedirs('outputs', exist_ok=True)\n",
    "    #    model_file = os.path.join('outputs', 'aerial_model_fine_tune.h5')\n",
    "    #    model.save(model_file)\n",
    "\n",
    "    \n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_list(\"train_files\", None, \"Training TFRecord files\")\n",
    "flags.DEFINE_list(\"eval_files\", None, \"Evaluation TFRecord files\")\n",
    "\n",
    "flags.DEFINE_integer(\"epochs\", 5, \"Number of epochs to train\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"Batch size\")\n",
    "flags.DEFINE_integer(\"steps_per_epoch\", 1000, \"Steps per epoch\")\n",
    "flags.DEFINE_integer(\"validation_steps\", 20, \"Batch size\")\n",
    "\n",
    "flags.DEFINE_string(\"job-dir\", None, \"Job dir\")\n",
    "\n",
    "# Required flags\n",
    "flags.mark_flag_as_required(\"train_files\")\n",
    "flags.mark_flag_as_required(\"eval_files\")\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv #Unused\n",
    "    \n",
    "    train_evaluate()\n",
    "     \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='sandbox-235500'\n",
    "IMAGE_REPO_NAME='horovod'\n",
    "IMAGE_TAG='gpu'\n",
    "IMAGE_URI='gcr.io/' + PROJECT_ID + '/' + IMAGE_REPO_NAME + ':' + IMAGE_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/sandbox-235500/horovod:gpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  59.39kB\n",
      "Step 1/21 : FROM nvidia/cuda:9.0-devel-ubuntu16.04\n",
      " ---> c24bd4961e81\n",
      "Step 2/21 : ENV TENSORFLOW_VERSION=1.12.0\n",
      " ---> Using cache\n",
      " ---> 69f25775acd2\n",
      "Step 3/21 : ENV CUDNN_VERSION=7.4.1.5-1+cuda9.0\n",
      " ---> Using cache\n",
      " ---> 1f6e7a9c6153\n",
      "Step 4/21 : ENV NCCL_VERSION=2.3.7-1+cuda9.0\n",
      " ---> Using cache\n",
      " ---> 0006b651299d\n",
      "Step 5/21 : ARG python=3.5\n",
      " ---> Using cache\n",
      " ---> 42b54bdb5ef5\n",
      "Step 6/21 : ENV PYTHON_VERSION=${python}\n",
      " ---> Using cache\n",
      " ---> e47eeaf95b73\n",
      "Step 7/21 : RUN apt-get update && apt-get install -y --allow-downgrades --allow-change-held-packages --no-install-recommends         build-essential         cmake         git         curl         vim         wget         jq         ca-certificates         libcudnn7=${CUDNN_VERSION}         libnccl2=${NCCL_VERSION}         libnccl-dev=${NCCL_VERSION}         libjpeg-dev         libpng-dev         python${PYTHON_VERSION}         python${PYTHON_VERSION}-dev\n",
      " ---> Using cache\n",
      " ---> bea2f239d7a2\n",
      "Step 8/21 : RUN ln -s /usr/bin/python${PYTHON_VERSION} /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> 531af5eff6e6\n",
      "Step 9/21 : RUN curl -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py\n",
      " ---> Using cache\n",
      " ---> a90101cd5a6d\n",
      "Step 10/21 : RUN pip install 'numpy<1.15.0' tensorflow-gpu==${TENSORFLOW_VERSION}\n",
      " ---> Using cache\n",
      " ---> a74af27443f0\n",
      "Step 11/21 : RUN mkdir /tmp/openmpi &&     cd /tmp/openmpi &&     wget https://www.open-mpi.org/software/ompi/v4.0/downloads/openmpi-4.0.0.tar.gz &&     tar zxf openmpi-4.0.0.tar.gz &&     cd openmpi-4.0.0 &&     ./configure --enable-orterun-prefix-by-default &&     make -j $(nproc) all &&     make install &&     ldconfig &&     rm -rf /tmp/openmpi\n",
      " ---> Using cache\n",
      " ---> 45c46d52528f\n",
      "Step 12/21 : RUN ldconfig /usr/local/cuda-9.0/targets/x86_64-linux/lib/stubs &&     HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_WITH_TENSORFLOW=1 pip install --no-cache-dir horovod &&     ldconfig\n",
      " ---> Using cache\n",
      " ---> 5c88d84ef8e3\n",
      "Step 13/21 : RUN apt-get install -y --no-install-recommends openssh-client openssh-server &&     mkdir -p /var/run/sshd\n",
      " ---> Using cache\n",
      " ---> 3f6bbf9c185f\n",
      "Step 14/21 : RUN cat /etc/ssh/ssh_config | grep -v StrictHostKeyChecking > /etc/ssh/ssh_config.new &&     echo \"    StrictHostKeyChecking no\" >> /etc/ssh/ssh_config.new &&     mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config\n",
      " ---> Using cache\n",
      " ---> 0ebcbf43d484\n",
      "Step 15/21 : RUN apt-get install -y --no-install-recommends subversion &&     svn checkout https://github.com/horovod/horovod/trunk/examples &&     rm -rf /examples/.svn\n",
      " ---> Using cache\n",
      " ---> d6b4cf3492d4\n",
      "Step 16/21 : WORKDIR \"/examples\"\n",
      " ---> Using cache\n",
      " ---> a12b42a368f3\n",
      "Step 17/21 : RUN mkdir /scripts\n",
      " ---> Using cache\n",
      " ---> 03409a82eca5\n",
      "Step 18/21 : COPY train /scripts\n",
      " ---> Using cache\n",
      " ---> 5f24f5375454\n",
      "Step 19/21 : WORKDIR \"/scripts\"\n",
      " ---> Using cache\n",
      " ---> 05480ffd9cb3\n",
      "Step 20/21 : COPY ./docker-entrypoint.sh /\n",
      " ---> 8f1301143305\n",
      "Step 21/21 : ENTRYPOINT [\"/docker-entrypoint.sh\"]\n",
      " ---> Running in eb8907985579\n",
      "Removing intermediate container eb8907985579\n",
      " ---> e1cd146da432\n",
      "Successfully built e1cd146da432\n",
      "Successfully tagged gcr.io/sandbox-235500/horovod:gpu\n"
     ]
    }
   ],
   "source": [
    "!docker build -f Dockerfile -t $IMAGE_URI ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA='gs://jkdatasets/cifar10/cifar10-train.tfrecord-00000-of-00010,gs://jkdatasets/cifar10/cifar10-train.tfrecord-00001-of-00010'\n",
    "EVAL_DATA='gs://jkdatasets/cifar10/cifar10-test.tfrecord-00000-of-00001'\n",
    "JOB_DIR='/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm --runtime=nvidia $IMAGE_URI \\\n",
    "--train_files=$TRAIN_DATA \\\n",
    "--eval_files=$EVAL_DATA \\\n",
    "--epochs=2 \\\n",
    "--job-dir=$JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/sandbox-235500/horovod]\n",
      "\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[21Bgpu: digest: sha256:111fd7f2b084bbb842bdcdc5b96a06b02408b0613242f6c045d329eef8efbd9a size: 4722\n"
     ]
    }
   ],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the job on CMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "REGION='us-west1'\n",
    "BUCKET_NAME='gs://jkcmle'\n",
    "JOB_NAME = 'horovod_' + datetime.datetime.today().strftime('%Y%M%d_%H%M%S')\n",
    "JOB_DIR=BUCKET_NAME + '/jobs/' + JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [horovod_20194702_054716] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe horovod_20194702_054716\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs horovod_20194702_054716\n",
      "jobId: horovod_20194702_054716\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "REGION='us-west1'\n",
    "BUCKET_NAME='gs://jkcmle'\n",
    "JOB_NAME = 'horovod_' + datetime.datetime.today().strftime('%Y%M%d_%H%M%S')\n",
    "JOB_DIR=BUCKET_NAME + '/jobs/' + JOB_NAME\n",
    "\n",
    "!gcloud beta ml-engine jobs submit training $JOB_NAME \\\n",
    "--region $REGION \\\n",
    "--scale-tier custom \\\n",
    "--master-image-uri $IMAGE_URI \\\n",
    "--master-machine-type standard \\\n",
    "--worker-image-uri $IMAGE_URI \\\n",
    "--worker-machine-type standard \\\n",
    "--worker-server-count 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ml-engine jobs submit training $JOB_NAME \\\n",
    "--region $REGION \\\n",
    "--master-image-uri $IMAGE_URI \\\n",
    "--scale-tier BASIC_GPU \\\n",
    "-- \\\n",
    "--train_files $TRAIN_DATA \\\n",
    "--eval_files $EVAL_DATA \\\n",
    "--epochs 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
